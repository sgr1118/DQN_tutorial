{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BvlTk87R_M4E2EmgB6ne2AqDuqjfrANd",
      "authorship_tag": "ABX9TyMcCF/PUlk2fGxGno4qo0zb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgr1118/DQN_tutorial/blob/main/DQN_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 강화학습 튜토리얼\n",
        "\n",
        "### 이 튜토리얼에서는 OpenAI Gym 의 CartPole-v0 태스크에서 DQN (Deep Q Learning) 에이전트를 학습하는데 PyTorch를 사용하는 방법을 보여드립니다.\n",
        "\n",
        "##### [원본 링크](https://tutorials.pytorch.kr/intermediate/reinforcement_q_learning.html)\n",
        "\n",
        "태스크\n",
        "\n",
        "에이전트는 연결된 막대가 똑바로 서 있도록 카트를 왼쪽이나 오른쪽으로 움직이는 두 가지 동작 중 하나를 선택해야 합니다. 다양한 알고리즘과 시각화 기능을 갖춘 공식 순위표를 Gym 웹사이트 에서 찾을 수 있습니다\n",
        "\n",
        "![](https://tutorials.pytorch.kr/_images/cartpole.gif)\n",
        "<center></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "crTsGpLnR_4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "KsiSPeeYQ03v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y7uungd9Ru7G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f329fd2a-f1fd-4767-fce7-43cc712caa17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "# 라이브러리 불러오기\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "if gym.__version__ < '0.26':\n",
        "    env = gym.make('CartPole-v0', new_step_api = True, \n",
        "                   render_mode = 'single_rgb_array').unwrapped\n",
        "else:\n",
        "    env = gym.make('CartPole-v0', render_mode = 'rgb_array').unwrapped\n",
        "\n",
        "# 시각화 설정\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# GPU 사용 여부 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 재현 메모리\n",
        "- 강화 학습을 위해 경험 재현 메모리를 사용할 것이다. 에이전트가 관찰한 전환(transition)을 저장하고 나중에 이 데이터를 재사용할 수 있다. 무작위로 샘플링하면 배치를 구성하는 전환들이 비상관(decorrelated)하게 됩니다. 이것이 강화 학습 절차를 크게 안정시키고 향상시키는 것으로 나타났습니다.\n",
        "\n",
        "### 필요한 클래스\n",
        " - Transition - 우리 환경에서 단일 전환을 나타내도록 명명된 튜플. 그것은 화면의 차이인 state로 (state, action) 쌍을 (next_state, reward) 결과로 매핑합니다.\n",
        "\n",
        " - ReplayMemory - 최근 관찰된 전이를 보관 유지하는 제한된 크기의 순환 버퍼. 또한 학습을 위한 전환의 무작위 배치를 선택하기위한 .sample () 메소드를 구현합니다."
      ],
      "metadata": {
        "id": "a2WdffZHy_aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 재현 메모리 Code\n",
        "Transition = namedtuple('Transition', ('state', \n",
        "                        'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    \n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen = capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"transition 저장\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "IShTIl_EzmUG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 강화학습 알고리즘\n",
        "- 우리의 환경은 결정론적이므로 여기에 제시된 모든 방정식은 단순화를 위해\n",
        "결정론적으로 공식화됩니다. 강화 학습 자료은 환경에서 확률론적 전환에\n",
        "대한 기대값(expectation)도 포함할 것입니다.\n",
        "\n",
        "우리의 목표는 할인된 누적 보상 (discounted cumulative reward)을\n",
        "극대화하려는 정책(policy)을 학습하는 것입니다.\n",
        "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, 여기서\n",
        "$R_{t_0}$ 는 *반환(return)* 입니다. 할인 상수,\n",
        "$\\gamma$, 는 $0$ 과 $1$ 의 상수이고 합계가\n",
        "수렴되도록 보장합니다. 에이전트에게 불확실한 먼 미래의 보상이\n",
        "가까운 미래의 것에 비해 덜 중요하게 만들고, 이것은 상당히 합리적입니다.\n",
        "\n",
        "Q-learning의 주요 아이디어는 만일 함수 $Q^*: State \\times Action \\rightarrow \\mathbb{R}$ 를\n",
        "가지고 있다면 반환이 어떻게 될지 알려줄 수 있고,\n",
        "만약 주어진 상태(state)에서 행동(action)을 한다면, 보상을 최대화하는\n",
        "정책을 쉽게 구축할 수 있습니다:\n",
        "\n",
        "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
        "\n",
        "그러나 세계(world)에 관한 모든 것을 알지 못하기 때문에,\n",
        "$Q^*$ 에 도달할 수 없습니다. 그러나 신경망은\n",
        "범용 함수 근사자(universal function approximator)이기 때문에\n",
        "간단하게 생성하고 $Q^*$ 를 닮도록 학습할 수 있습니다.\n",
        "\n",
        "학습 업데이트 규칙으로, 일부 정책을 위한 모든 $Q$ 함수가\n",
        "Bellman 방정식을 준수한다는 사실을 사용할 것입니다:\n",
        "\n",
        "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
        "\n",
        "평등(equality)의 두 측면 사이의 차이는\n",
        "시간차 오류(temporal difference error), $\\delta$ 입니다.:\n",
        "\n",
        "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
        "\n",
        "오류를 최소화하기 위해서 [Huber\n",
        "loss](https://en.wikipedia.org/wiki/Huber_loss)_ 를 사용합니다.\n",
        "Huber loss 는 오류가 작으면 평균 제곱 오차( mean squared error)와 같이\n",
        "동작하고 오류가 클 때는 평균 절대 오류와 유사합니다.\n",
        "- 이것은 $Q$ 의 추정이 매우 혼란스러울 때 이상 값에 더 강건하게 합니다.\n",
        "재현 메모리에서 샘플링한 전환 배치 $B$ 에서 이것을 계산합니다:\n",
        "\n",
        "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
        "\n",
        "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
        "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
        "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
        "   \\end{cases}\\end{align}"
      ],
      "metadata": {
        "id": "3K60Cyxf05SE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-네트워크\n",
        "- 우리 모델은 현재와 이전 스크린 패치의 차이를 취하는 CNN이다. 두가지 출력 $Q(s, left)$와 $Q(s, right)$가 있다. 여기서 s는 네트워크의 입력이다. 결과적으로 네트워크는 주어진 현재 입력에서 각 행동의 기대값을 예측하려고 한다."
      ],
      "metadata": {
        "id": "oMdCC18q26k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-네트워크 모델\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size = 5, stride = 2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size = 5, stride = 2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size = 5, stride = 2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Linear 입력의 연결 숫자는 conv2d 계층의 출력과 입력 이미지의 크기에\n",
        "        # 따라 결정되기 때문에 따로 계산을 해야합니다.\n",
        "\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) -1) // stride + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "        # 최적화 중에 다음 행동을 결정하기 위해서 하나의 요소 또는 배치를 이용해 호촐됩니다.\n",
        "        # ([[left0exp,right0exp]...]) 를 반환합니다.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "metadata": {
        "id": "iTzIVlKg3PCN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 입력 추출\n",
        "- 아래 코드는 환경에서 렌더링 된 이미지를 추출하고 처리하는 유틸리티입니다. 이미지 변환을 쉽게 구성할 수 있는 torchvision 패키지를 사용합니다. 셀(cell)을 실행하면 추출한 예제 패치가 표시됩니다."
      ],
      "metadata": {
        "id": "zfNccdyX4yIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 추출 Code\n",
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation = Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0) # MIDDLE OF CART\n",
        "\n",
        "def get_screen():\n",
        "    # gym이 요청한 화면은 400x600x3 이지만, 가끔 800x1200x3 처럼 큰 경우가 있습니다.\n",
        "    # 이것을 Torch order (CHW)로 변환한다.\n",
        "    screen = env.render().transpose((2,0,1))\n",
        "    # 카트는 아래쪽에 있으므로 화면의 상단과 하단을 제거\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height * 0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "        \n",
        "    # 카트를 중심으로 정사각형 이미지가 되도록 가장자리를 제거\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # float 으로 변환하고,  rescale 하고, torch tensor 로 변환하십시오.\n",
        "\n",
        "    screen = np.ascontiguousarray(screen, dtype = np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "\n",
        "    # 크기를 수정하고 배치 차원(BCHW)를 추가\n",
        "    return resize(screen).unsqueeze(0)\n",
        "\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1,2,0).numpy(),\n",
        "           interpolation='none') # 이미지 출력\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "r4gVesOt4157",
        "outputId": "6c7c17b4-39e5-4195-bcd1-8cce97f93caf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습\n",
        "- 이 부분은 모델과 최적화기를 인스턴스화하고 일부 유틸리티를 정의한다.\n",
        "\n",
        "- select_action - Epsilon Greedy 정책에 따라 행동을 선택합니다. 간단히 말해서, 가끔 모델을 사용하여 행동을 선택하고 때로는 단지 하나를 균일하게 샘플링할 것입니다. 임의의 액션을 선택할 확률은 EPS_START 에서 시작해서 EPS_END 를 향해 지수적으로 감소할 것입니다. EPS_DECAY 는 감쇠 속도를 제어합니다.\n",
        "\n",
        "- plot_durations - 지난 100개 에피소드의 평균(공식 평가에서 사용 된 수치)에 따른 에피소드의 지속을 도표로 그리기 위한 헬퍼. 도표는 기본 훈련 루프가 포함 된 셀 밑에 있으며, 매 에피소드마다 업데이트됩니다."
      ],
      "metadata": {
        "id": "0_-xZ_PP-Lnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 Code\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "# AI gym에서 반환된 형태를 기반으로 계층을 초기화 하도록 화면의 크기를\n",
        "# 가져옵니다. 이 시점에 일반적으로 3x40x90 에 가깝습니다.\n",
        "# 이 크기는 get_screen()에서 고정, 축소된 렌더 버퍼의 결과입니다.\n",
        "\n",
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "steps_done = 0 # 초기 Step\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "    math.exp(-1 * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
        "            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
        "            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
        "            return policy_net(state).max(1)[1].view(1,1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device = device,\n",
        "                            dtype = torch.long)\n",
        "        \n",
        "episode_durations = []\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype = torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001) # 도표가 업데이트할 수 있게 잠시 멈춘다.\n",
        "    if is_ipython:\n",
        "        display.clear_output(wait = True)\n",
        "        display.display(plt.gcf())"
      ],
      "metadata": {
        "id": "ClTVD36T-fxv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 루프\n",
        "- 최종적으로 모델 학습을 위한 코드\n",
        "\n",
        "- 여기서, 최적화하는 한 단계를 수행하는 optimize_model 함수가 있다. 먼저 배치 하나를 샘플링하고 모든 Tensor를 하나로 연결하고 $Q(s_t, a_t), V(s_{t+1}) = max_aQ(s_{t+1}, a)$를 게산하고 그것들을 손실로 합칩니다. 우리가 설정한 정의에 따르면 만약 $s$가 마지막 상태라면 $V(s) = O$입니다.\n",
        "\n",
        "- 또한 안정성을 추가하기 위한 $V(s_{t+1})$게산을 위해 목표 네트워크를 사용한다. 목표 네트워크는 대부분의 시간 동결 상태로 유지되지만, 가끔 정책 네트워크의 가중치로 업데이트된다. 이것은 대개 설정한 스텝 숫자이지만 단순화를 위한 에피소드를 사용합니다."
      ],
      "metadata": {
        "id": "6yjV3LSlEWLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 루프 Code\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\n",
        "    # 전환합니다.\n",
        "\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다\n",
        "    # (최종 상태는 시뮬레이션이 종료 된 이후의 상태)\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                            batch.next_state)), device = device,\n",
        "                                  dtype = torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                       if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
        "    # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
        "    # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
        "    # max(1)[0]으로 최고의 보상을 선택하십시오.\n",
        "    # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다..\n",
        "\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device = device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "\n",
        "    # 기대 Q 값 계산\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Huber 손실 계산\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # 모델 최적화\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "6Fnvw3hzGkOP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 아래에서 code는 주요 학습 루프를 찾을 수 있다. 처음으로 환경을 재설정하고 상태 Tensor를 초기화한다. 그런 다음 행동을 샘플링하고, 그것을 실행하고, 다음 화면과 보상(항상 1)을 관찰하고, 모델을 한 번 최적화합니다. 에피소드가 끝나면 (모델이 실패)루프를 다시 시작한다.\n",
        "\n",
        "- 아래에서 num_episodes는 작게 설정됩니다. 노트북을 다운받고 의미있는 개선을 위해서 300이상의 에피소드도 실행해본다."
      ],
      "metadata": {
        "id": "rht1w07JK9QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 주요 학습 루프 찾기 Code\n",
        "num_episodes = 300\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # 환경과 상태 초기화\n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen\n",
        "    for t in count():\n",
        "        # 행동 선택과 수행\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device = device)\n",
        "\n",
        "        # 새로운 상태 관찰\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # 메모리에 변이 저장\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # 다음 상태로 이동\n",
        "        state = next_state\n",
        "\n",
        "        # (정책 네트워크에서) 최적화 단계 수행\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            episode_durations.append( t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "    # 목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "XL1PxI7mLuvc",
        "outputId": "8ad5e342-e9ad-462d-fb83-c019354a0211"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://tutorials.pytorch.kr/_images/reinforcement_learning_diagram.jpg)\n",
        "<center>전체 결과 데이터 흐름을 보여주는 다이어그램</center>\n",
        "\n",
        "- 행동은 무작위 또는 정책에 따라 선택되어, gym 환경애서 다음 단계 샘플을 가져옵니다. 결과를 재현 메모리에 저장하고 모든 반복에서 최적화 단계를 실행\n",
        "\n",
        "- 최적화는 재현 메모리에서 무작위 배치를 선택하여 새 정책을 학습합니다. 이전 target_net은 최적화에서 기대 Q 값을 계산하는 데에도 사용되고, 최신 상태를 유지하기 위해 가끔 업데이트 된다."
      ],
      "metadata": {
        "id": "4V4f00kbVk1Z"
      }
    }
  ]
}